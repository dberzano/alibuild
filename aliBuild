#!/usr/bin/env python
from argparse import ArgumentParser
import os, sys, shutil, subprocess, logging, time, re
from commands import getstatusoutput
from os.path import basename, dirname, abspath, exists, realpath
from yaml import safe_load
from os import makedirs, unlink
from hashlib import sha1
from glob import glob
from os import readlink
from logging import debug,error
from datetime import datetime

def format(s, **kwds):
  return s % kwds

def star():
  return basename(sys.argv[0]).replace("Build", "")

def gzip():
  return getstatusoutput("which pigz")[0] and "gzip" or pigz

def execute(command, printer=debug):
  popen = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)
  lines_iterator = iter(popen.stdout.readline, "")
  for line in lines_iterator:
    debug(line.strip("\n"))  # yield line
  output = popen.communicate()[0]
  debug(output)
  exitCode = popen.returncode
  return exitCode

def dieOnError(error, msg):
  if error:
    print msg
    sys.exit(1)

def updateReferenceRepos(referenceSources, p, spec):
  # Update source reference area, if possible.
  # If the area is already there and cannot be written, assume it maintained
  # by someone else.
  #
  # If the area can be created, clone a bare repository with the sources.
  debug("Updating references.")
  referenceRepo = "%s/%s" % (abspath(referenceSources), p.lower())
  if os.access(dirname(referenceSources), os.W_OK):
    getstatusoutput("mkdir -p %s" % referenceSources)
  writeableReference = os.access(referenceSources, os.W_OK)
  if not writeableReference and exists(referenceRepo):
    debug("Using %s as reference for %s." % (referenceRepo, p))
    spec["reference"] = referenceRepo
    return
  if not writeableReference:
    debug("Cannot create reference for %s in specified folder.", p)
    return

  err, out = getstatusoutput("mkdir -p %s" % abspath(referenceSources))
  if not "source" in spec:
    return
  if not exists(referenceRepo):
    cmd = ["git", "clone", "--bare", spec["source"], referenceRepo]
    debug(" ".join(cmd))
    err = execute(" ".join(cmd))
  else:
    err = execute(format("cd %(referenceRepo)s && "
                         "git fetch --tags %(source)s 2>&1 && "
                         "git fetch %(source)s 2>&1",
                         referenceRepo=referenceRepo,
                         source=spec["source"]))
  if err:
    print "Error while updating reference repos %s." % spec["source"]
    sys.exit(1)
  spec["reference"] = referenceRepo

def getDirectoryHash(d):
  err, out = getstatusoutput("GIT_DIR=%s/.git git show-ref HEAD | cut -f1 -d\ " % d)
  if err:
    print "Impossible to find reference for %s " % d
    sys.exit(0)
  return out

if __name__ == "__main__":
  parser = ArgumentParser()
  parser.add_argument("action")
  parser.add_argument("pkgname")
  parser.add_argument("--config-dir", "-c", dest="configDir", default="%sdist" % star())
  parser.add_argument("--work-dir", "-w", dest="workDir", default="sw")
  parser.add_argument("--architecture", "-a", dest="architecture", required=True)
  parser.add_argument("--jobs", "-j", dest="jobs", default=1)
  parser.add_argument("--reference-sources", dest="referenceSources", default="sw/MIRROR")
  parser.add_argument("--remote-store", dest="remoteStore", default="",
                      help="Where to find packages already built for reuse."
                           "Use ssh:// in front for remote store. End with ::rw if you want to upload.")
  parser.add_argument("--write-store", dest="writeStore", default="",
                      help="Where to upload the built packages for reuse."
                           "Use ssh:// in front for remote store.")
  parser.add_argument("--debug", "-d", dest="debug", action="store_true", default=False)
  args = parser.parse_args()

  logger = logging.getLogger()
  logger_handler = logging.StreamHandler()
  logger.addHandler(logger_handler)

  if args.debug:
    logger.setLevel(logging.DEBUG)
    logger_handler.setFormatter(logging.Formatter('%(levelname)s: %(message)s'))
  else:
    logger.setLevel(logging.INFO)

  rsyncOptions = ""
  if args.remoteStore.startswith("ssh://"):
    args.remoteStore = args.remoteStore[6:]
  if args.writeStore.startswith("ssh://"):
    args.writeStore = args.writeStore[6:]
  if args.remoteStore.endswith("::rw") and args.writeStore:
    print "You cannot specify ::rw and --write-store at the same time"
    sys.exit(1)
  if args.remoteStore.endswith("::rw"):
    args.remoteStore = args.remoteStore[0:-4]
    args.writeStore = args.remoteStore

  # Setup build environment.
  if not args.action == "build":
    parser.error("Action %s unsupported" % args.action)
  packages = [args.pkgname]
  specs = {}
  buildOrder = []
  workDir = abspath(args.workDir)
  specDir = "%s/SPECS" % workDir
  if not exists(specDir):
    makedirs(specDir)

  debug(format("Using %(star)sBuild from "
               "%(star)sbuild@%(toolHash)s recipes "
               "in %(star)sdist@%(distHash)s",
               star=star(),
               toolHash=getDirectoryHash(dirname(__file__)),
               distHash=getDirectoryHash(args.configDir)))

  while packages:
    p = packages.pop(0)
    if p in specs:
      buildOrder.remove(p)
      buildOrder.insert(0, p)
      continue
    try:
      d = open("%s/%s.sh" % (args.configDir, p.lower())).read()
    except IOError,e:
      print e
      sys.exit(1)
    commentHeader, recipe = d.split("#---", 1)
    header = ""
    for headerLine in commentHeader.split("\n"):
      header = header + re.sub(r"^\s*#", "", headerLine) + "\n"
    spec = safe_load(header)
    # Check that version is a string
    if not isinstance(spec["version"], basestring):
      print "In recipe \"%s\": version must be a string" % p
      sys.exit(1)
    spec["tag"] = spec.get("tag", spec["version"])
    spec["version"] = spec["version"].replace("/", "_")
    spec["recipe"] = recipe.strip("\n")
    specs[spec["package"]] = spec
    buildOrder.insert(0, spec["package"])
    packages += spec.get('requires', [])
    open("%s/SPECS/%s.sh" % (workDir, spec["package"]), "w").write(spec["recipe"])

  # Resolve the tag to the actual commit ref, so that
  for p in buildOrder:
    spec = specs[p]
    spec["commit_hash"] = "0"
    if "source" in spec:
      cmd = format("git ls-remote --heads %(source)s",
                   source = spec["source"])
      err, out = getstatusoutput(cmd)
      if err:
        print "Unable to fetch from %s" % spec["source"]
        sys.exit(1)
      # by default we assume tag is a commit hash
      spec["commit_hash"] = spec["tag"]
      for l in out.split("\n"):
        if l.endswith("refs/heads/%s" % spec["tag"]):
          spec["commit_hash"] = l.split("\t", 1)[0]
          break

  # Decide what is the main package we are building and at what commit.
  #
  # We emit an event for the main package, when encountered, so that we can use
  # it to index builds of the same hash on different architectures. We also
  # make sure add the main package and it's hash to the debug log, so that we
  # can always extract it from it.
  # If one of the special packages is in the list of packages to be built,
  # we use it as main package, rather than the last one.
  mainPackage = None
  mainHash = None
  for p in buildOrder:
    spec = specs[p]
    mainPackage = p
    mainHash = spec["commit_hash"]
    if p.lower() in ["aliroot", "aliphysics", "o2"]:
      break
  debug("Main package is %s@%s" % (mainPackage, mainHash))
  if args.debug:
    logger_handler.setFormatter(
        logging.Formatter("%%(levelname)s:%s:%s: %%(message)s" %
                          (mainPackage, mainHash[0:8])))

  # Now that we have the main package set, we can print out Useful information
  # which we will be able to associate with this build.
  for p in buildOrder:
    spec = specs[p]
    if "source" in spec:
      debug("Commit hash for %s@%s is %s" % (spec["source"], spec["tag"], spec["commit_hash"]))

  # Expand variables found in the version.  You can define things like date
  # with %(year)s %(month)s %(day)s %(hour)s and you can refer to some of the
  # other properties like %(tag)s %(commit_hash)s
  now = datetime.now()
  for p in buildOrder:
    spec = specs[p]
    spec["version"] = format(spec["version"],
                             commit_hash=spec["commit_hash"],
                             short_hash=spec["commit_hash"][0:10],
                             tag=spec["tag"],
                             year=str(now.year),
                             month=str(now.month),
                             day=str(now.day),
                             hour=str(now.hour))

  # Calculate the hashes. We do this in build order so that we can guarantee
  # that the hashes of the dependencies are calculated first.  Also notice that
  # if the commit hash is a real hash, and not a tag, we can safely assume
  # that's unique, and therefore we can avoid putting the repository or the
  # name of the branch in the hash.
  debug("Calculating hashes.")
  for p in buildOrder:
    spec = specs[p]
    h = sha1()
    for x in ["recipe", "version", "package", "commit_hash",
              "env", "append_path", "prepend_path"]:
      h.update(str(spec.get(x, "none")))
    if spec["commit_hash"] == spec.get("tag", "0"):
      h.update(spec.get("source", "none"))
      if "source" in spec:
        h.update(spec["tag"])
    for dep in spec.get("requires", []):
      h.update(specs[dep]["hash"])
    if bool(spec.get("force_rebuild", False)):
      h.update(str(time.time()))
    spec["hash"] = h.hexdigest()
    debug("Hash for recipe %s is %s" % (p, spec["hash"]))

  # This adds to the spec where it should find, locally or remotely the
  # various tarballs and links.
  for p in buildOrder:
    spec = specs[p]
    pkgSpec = {
      "repo": workDir,
      "package": spec["package"],
      "version": spec["version"],
      "hash": spec["hash"],
      "prefix": spec["hash"][0:2],
      "architecture": args.architecture
    }
    tarSpecs = [
      ("storePath", "TARS/%(architecture)s/store/%(prefix)s/%(hash)s"),
      ("linksPath", "TARS/%(architecture)s/%(package)s"),
      ("tarballHashDir", "%(repo)s/TARS/%(architecture)s/store/%(prefix)s/%(hash)s"),
      ("tarballLinkDir", "%(repo)s/TARS/%(architecture)s/%(package)s")
    ]
    spec.update(dict([(x, format(y, **pkgSpec)) for (x, y) in tarSpecs]))

  for p in buildOrder:
    spec = specs[p]
    todo = [p]
    spec["full_requires"] = []
    while todo:
      requires = specs[todo.pop(0)].get("requires", [])
      spec["full_requires"] += requires
      todo += requires
    spec["full_requires"] = set(spec["full_requires"])

  # We now iterate on all the packages, making sure we build correctly every
  # single one of them. This is done this way so that the second time we run we
  # can check if the build was consistent and if it is, we bail out.
  packageIterations = 0
  while buildOrder:
    packageIterations += 1
    if packageIterations > 20:
      print "Too many attempts at building %s. Something wrong with the repository?"
      exit(1)
    p = buildOrder[0]
    spec = specs[p]
    # Since we can execute this multiple times for a given package, in order to
    # ensure consistency, we need to reset things and make them pristine.
    spec.pop("revision", None)

    debug("Updating from tarballs")
    # If we arrived here it really means we have a tarball which was created
    # using the same recipe. We will use it as a cache for the build. This means
    # that while we will still perform the build process, rather than
    # executing the build itself we will:
    #
    # - Unpack it in a temporary place.
    # - Invoke the relocation specifying the correct work_dir and the
    #   correct path which should have been used.
    # - Move the version directory to its final destination, including the
    #   correct revision.
    # - Repack it and put it in the store with the
    #
    # this will result in a new package which has the same binary contents of
    # the old one but where the relocation will work for the new dictory. Here
    # we simply store the fact that we can reuse the contents of cachedTarball.
    if args.remoteStore:
      debug("Updating remote store for package %s@%s" % (p, spec["hash"]))
      cmd = format("mkdir -p %(tarballHashDir)s\n"
                   "rsync -av %(rsyncOptions)s %(remoteStore)s/%(storePath)s/ %(tarballHashDir)s/ || true\n"
                   "rsync -av --delete %(rsyncOptions)s %(remoteStore)s/%(linksPath)s/ %(tarballLinkDir)s/ || true\n",
                   rsyncOptions=rsyncOptions,
                   remoteStore=args.remoteStore,
                   storePath=spec["storePath"],
                   linksPath=spec["linksPath"],
                   tarballHashDir=spec["tarballHashDir"],
                   tarballLinkDir=spec["tarballLinkDir"])
      err = execute(cmd)
      dieOnError(err, "Unable to update from specified store.")
    # Decide how it should be called, based on the hash and what is already
    # available.
    debug("Checking for packages already built.")
    linksGlob = format("%(w)s/TARS/%(a)s/%(p)s/%(p)s-%(v)s-*.%(a)s.tar.gz",
                       w=workDir,
                       a=args.architecture,
                       p=spec["package"],
                       v=spec["version"])
    debug("Glob patter used: %s" % linksGlob)
    packages = glob(linksGlob)
    # In case there is no installed software, revision is 1
    # If there is already an installed package:
    # - Remove it if we do not know its hash
    # - Use the latest number in the version, to decide its revision
    debug("Packages already built using this version\n%s" % "\n".join(packages))
    if not packages:
      spec["revision"] = 1
    busyRevisions = []
    for d in packages:
      realPath = readlink(d)
      matcher = format("../../%(a)s/store/[0-9a-f]{2}/([0-9a-f]*)/%(p)s-%(v)s-([0-9]*).%(a)s.tar.gz",
                       a=args.architecture,
                       p=spec["package"],
                       v=spec["version"])
      m = re.match(matcher, realPath)
      if not m:
        continue
      h, revision = m.groups()
      revision = int(revision)
      # If we have an hash match, we use the old revision for the package
      # and we do not need to build it.
      if h == spec["hash"]:
        print "Package %s with hash %s is already found in %s. Not building." % (p, h, d)
        spec["revision"] = revision
        break
      else:
        busyRevisions.append(revision)

    if not "revision" in spec:
      spec["revision"] = min(set(range(1, max(busyRevisions)+2)) - set(busyRevisions))

    # Now that we have all the information about the package we want to build, let's
    # check if it wasn't built / unpacked already.
    hashFile = "%s/%s/%s/%s-%s/.build-hash" % (workDir, 
                                               args.architecture, 
                                               spec["package"],
                                               spec["version"],
                                               spec["revision"])
    try:
      fileHash = open(hashFile).read().strip("\n")
    except:
      fileHash = "0"
    if fileHash != spec["hash"]:
      if fileHash != "0":
        debug("Mismatch between local area and the one which I should build. Redoing.")
      shutil.rmtree(dirname(hashFile), True)
    else:
      # If we get here, we know we are in sync with whatever remote store.  We
      # can therefore create a directory which contains all the packages which
      # were used to compile this one.
      distributionLinks = []
      print spec["full_requires"]
      target = format("TARS/%(a)s/dist/%(p)s/%(p)s-%(v)s-%(r)s",
                      a=args.architecture,
                      p=spec["package"],
                      v=spec["version"],
                      r=spec["revision"])
      shutil.rmtree(target, True)
      for x in [spec["package"]] + list(spec["full_requires"]):
        dep = specs[x]
        source = format("../../../../../TARS/%(a)s/store/%(sh)s/%(h)s/%(p)s-%(v)s-%(r)s.%(a)s.tar.gz",
                        a=args.architecture,
                        sh=dep["hash"][0:2],
                        h=dep["hash"],
                        p=dep["package"],
                        v=dep["version"],
                        r=dep["revision"])
        err = execute(format("cd %(workDir)s &&"
                             "mkdir -p %(target)s &&"
                             "ln -sfn %(source)s %(target)s",
                             workDir = args.workDir,
                             target=target,
                             source=source))
      if args.writeStore:
        cmd = format("cd %(w)s && "
                     "rsync -avR %(o)s --ignore-existing %(t)s/  %(rs)s/",
                     w=args.workDir,
                     rs=args.writeStore,
                     o=rsyncOptions,
                     t=target)
        execute(cmd)
      buildOrder.pop(0)
      packageIterations = 0
      continue

    debug("Looking for cached tarball in %s" % spec["tarballHashDir"])
    # FIXME: I should get the tarballHashDir updated with server at this point.
    #        It does not really matter that the symlinks are ok at this point
    #        as I only used the tarballs as reusable binary blobs.
    spec["cachedTarball"] = (glob("%s/*" % spec["tarballHashDir"]) or [""])[0]
    debug(spec["cachedTarball"] and
          "Found tarball in %s" % spec["cachedTarball"] or
          "No cache tarballs found")

    # FIXME: Why doing it here? This should really be done before anything else.
    updateReferenceRepos(args.referenceSources, p, spec)

    # Generate the part which sources the environment for all the dependencies.
    # Notice that we guarantee that a dependency is always sourced before the
    # parts depending on it, but we do not guaranteed anything for the order in
    # which unrelated components are activated.
    dependencies = ""
    dependenciesInit = ""
    for dep in spec.get("requires", []):
      depSpec = specs[dep]
      depInfo = {
        "architecture": args.architecture,
        "package": dep,
        "version": depSpec["version"],
        "revision": depSpec["revision"],
        "bigpackage": dep.upper().replace("-", "_")
      }
      dependencies += format("source \"$WORK_DIR/%(architecture)s/%(package)s/%(version)s-%(revision)s/etc/profile.d/init.sh\"\n",
                             **depInfo)
      dependenciesInit += format('echo source \${WORK_DIR}/%(architecture)s/%(package)s/%(version)s-%(revision)s/etc/profile.d/init.sh >> \"$INSTALLROOT/etc/profile.d/init.sh\"\n',
                             **depInfo)
    # Generate the part which creates the environment for the package.
    # This can be either variable set via the "env" keyword in the metadata
    # or paths which get appended via the "append_path" one.
    # By default we append LD_LIBRARY_PATH, PATH and DYLD_LIBRARY_PATH
    # FIXME: do not append variables for Mac on Linux.
    environment = ""
    if type(spec.get("env", {})) != dict:
      print "Tag `env' in %s should be a dict." % p
      sys.exit(1)
    for (key, value) in spec.get("env", {}).items():
      environment += format("echo 'export %(key)s=%(value)s' >> $INSTALLROOT/etc/profile.d/init.sh\n",
                             architecture=args.architecture,
                             key=key,
                             value=value,
                           )
    basePath = "%s_ROOT" % p.upper().replace("-", "_")
    pathDict = spec.get("append_path", [])

    for pathEntry in pathDict:
      key, value = pathEntry.items()[0]
      environment += format("echo 'export %(key)s=$%(key)s:%(value)s' >> \"$INSTALLROOT/etc/profile.d/init.sh\"\n",
                             architecture=args.architecture,
                             key=key,
                             package=p,
                             revision=spec["revision"],
                             value=value,
                             version=spec["version"]
                           )

    # Same thing, but prepending the results so that they win against system ones.
    pathDict = [{"LD_LIBRARY_PATH": "$%s/lib" % basePath},
                {"DYLD_LIBRARY_PATH": "$%s/lib" % basePath},
                {"PATH": "$%s/bin" % basePath},
               ]
    pathDict += spec.get("prepend_path", [])

    for pathEntry in pathDict:
      key, value = pathEntry.items()[0]
      environment += format("echo 'export %(key)s=%(value)s:$%(key)s' >> \"$INSTALLROOT/etc/profile.d/init.sh\"\n",
                             architecture=args.architecture,
                             key=key,
                             package=p,
                             revision=spec["revision"],
                             value=value,
                             version=spec["version"]
                           )

    # The actual build script.
    referenceStatement = ""
    if "reference" in spec:
      referenceStatement = "export GIT_REFERENCE=%s" % spec["reference"]

    debug(spec)

    fp = open(dirname(realpath(__file__))+'/build_template.sh', 'r')
    cmd_raw = fp.read()
    fp.close()

    source = spec.get("source", "")
    # Shortend the commit hash in case it's a real commit hash and not simply
    # the tag.
    commit_hash = spec["commit_hash"]
    if spec["tag"] != spec["commit_hash"]:
      commit_hash = spec["commit_hash"][0:10]
    cmd = format(cmd_raw,
                 dependencies=dependencies,
                 dependenciesInit=dependenciesInit,
                 environment=environment,
                 workDir=workDir,
                 configDir=abspath(args.configDir),
                 pkgname=spec["package"],
                 hash=spec["hash"],
                 version=spec["version"],
                 revision=spec["revision"],
                 architecture=args.architecture,
                 jobs=args.jobs,
                 source=source,
                 write_repo=spec.get("write_repo", source),
                 tag=spec["tag"],
                 commit_hash=commit_hash,
                 referenceStatement=referenceStatement,
                 cachedTarball=spec["cachedTarball"],
                 gzip=gzip())
    scriptName = "%s/SPECS/%s/%s/%s-%s/build.sh" % (workDir,
                                                    args.architecture,
                                                    spec["package"],
                                                    spec["version"],
                                                    spec["revision"])
    err, out = getstatusoutput("mkdir -p %s" % dirname(scriptName))
    script = open(scriptName, "w")
    script.write(cmd)
    script.flush()
    script.close()

    print "Building %s@%s" % (spec["package"], spec["version"])
    err = execute("/bin/bash -e -x %s 2>&1" % scriptName)
    if err:
      print "Error while executing %s" % scriptName
      sys.exit(1)
    if args.writeStore:
      tarballNameWithRev = format("%(package)s-%(version)s-%(revision)s.%(architecture)s.tar.gz",
                                  architecture=args.architecture,
                                  **spec)
      cmd = format("cd %(workdir)s && "
                   "rsync -avR %(rsyncOptions)s --ignore-existing %(storePath)s/%(tarballNameWithRev)s  %(remoteStore)s/ &&"
                   "rsync -avR %(rsyncOptions)s --ignore-existing %(linksPath)s/%(tarballNameWithRev)s  %(remoteStore)s/",
                   workdir=args.workDir,
                   remoteStore=args.remoteStore,
                   rsyncOptions=rsyncOptions,
                   storePath=spec["storePath"],
                   linksPath=spec["linksPath"],
                   tarballNameWithRev=tarballNameWithRev)
      err = execute(cmd)
      dieOnError(err, "Unable to upload tarball.")
